<html>

<head>
  <meta charset="UTF-8">
  <title>Audio samples of "Multi-speaker Text-to-Speech Based on Deep Learning"</title>
  <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  <!-- <link rel="shortcut icon" href="images/taco.png"> -->
</head>

<body>
  <h1>
    Audio samples of "Multi-speaker Text-to-Speech Based on Deep Learning
  </h1>
  <div><b>Abstract:</b> 
    <p>
      With the rapid rise of computer science and artificial intelligence, Text-to-Speech (TTS) systems have achieved significant improvements. As one of the important research directions in TTS, multi-speaker speech synthesis technology still faces some problems such as large demand for the target speaker's corpus, high customization complexity and limited application scenarios. This paper focus on the above problems and carry out the following three parts of work.
    </p>
    <p>
      In the first part, this paper designed a single-speaker Chinese TTS system as the infrastructure for subsequent research. In order to solve the problems of poor quality and slow speech synthesis in traditional methods, this paper proposed a non-autoregressive speech synthesis system based on Fastspeech2 and Multi-band MelGAN. The experimental results demonstrated that the proposed system can achieve fast speech synthesis while maintaining the high quality of the synthesized speech.
    </p>
    <p>
      In the second part, in custom speech scenarios such as rapid sound bank customization and real-time speech imitation, this paper accomplished two multi-speaker TTS systems to make up for the shortcomings of traditional methods that require a large amount of target speaker corpus. The former system characterized speaker features by speaker ID and shared model parameters among speakers. The latter system used transfer learning to provide pre-trained speaker encoders for TTS systems, which can be customized for out-of-set speakers with only one audio.
    </p>
    <p>
      In the third part, in the cross-lingual speech synthesis scenario, this paper proposed a cross-lingual multi-speaker TTS system based on non-autoregressive framework to address the problem that the speaker similarity of the model decreases in cross-lingual synthesis. Further, in the multi-speaker TTS system based on transfer learning, domain adversarial training was introduced to facilitate the separation of textual and speaker information, which improved the speaker similarity of the synthesized speech.
    </p>
  </div>

  <article>
    <header>
      <span class="paper_date">Chapter3</span>
      <span class="paper_title">Single Speaker Chinese Speech Synthesis</span>
      <ul>
        <li><a href="chapters/single_speaker_chinese_tts/index.html">audio samples</a></li>
      </ul>
    </header>
  </article>

  <article>
    <header>
      <span class="paper_date">Chapter4</span>
      <span class="paper_title">Multi-Speaker Speech Synthesis</span>
      <ul>
        <li><a href="chapters/multi_speaker_tts/index.html">audio samples</a></li>
      </ul>
    </header>
  </article>

  <article>
    <header>
      <span class="paper_date">Chapter5</span>
      <span class="paper_title">Cross-Language Multi-Speaker Speech Synthesis Based on Domain Adversarial</span>
      <ul>
        <li><a href="chapters/cross_language_multi_speaker_tts/index.html">audio samples</a></li>
      </ul>
    </header>
  </article>


</body>

</html>